{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the URL\n",
    "url = \"https://www.indeed.com/jobs?q=python&l=Remote&ts=1727179231724&from=searchOnHP&rq=1&rsIdx=0&fromage=last&vjk=52108bd7282a4c2d\"\n",
    "result = requests.get(url)\n",
    "\n",
    "# Save page content/markup\n",
    "src = result.content\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "# Find all job titles by targeting the correct class name and nested span\n",
    "job_titles = soup.find_all('h2', {'class': 'jobTitle css-198pbd eu4oa1w0'})\n",
    "\n",
    "# Extract the text from the <span> inside the <h2>\n",
    "job_title_list = [title.find('span').text.strip() for title in job_titles]\n",
    "\n",
    "# Print the job titles\n",
    "print(job_title_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x00007FF750ACFDA5+29557]\n\t(No symbol) [0x00007FF750A42240]\n\t(No symbol) [0x00007FF7508FB6EA]\n\t(No symbol) [0x00007FF7508CFCD5]\n\t(No symbol) [0x00007FF75097F167]\n\t(No symbol) [0x00007FF7509981A1]\n\t(No symbol) [0x00007FF7509772A3]\n\t(No symbol) [0x00007FF7509412DF]\n\t(No symbol) [0x00007FF750942451]\n\tGetHandleVerifier [0x00007FF750DFDCBD+3363469]\n\tGetHandleVerifier [0x00007FF750E49B47+3674391]\n\tGetHandleVerifier [0x00007FF750E3EAEB+3629243]\n\tGetHandleVerifier [0x00007FF750B8FC66+815670]\n\t(No symbol) [0x00007FF750A4D6EF]\n\t(No symbol) [0x00007FF750A492B4]\n\t(No symbol) [0x00007FF750A49450]\n\t(No symbol) [0x00007FF750A381FF]\n\tBaseThreadInitThunk [0x00007FFAC2DA7374+20]\n\tRtlUserThreadStart [0x00007FFAC315CC91+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Find all job titles on the page\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m job_titles \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjobTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Extract and print job titles\u001b[39;00m\n\u001b[0;32m     24\u001b[0m job_title_list \u001b[38;5;241m=\u001b[39m [title\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m job_titles]\n",
      "File \u001b[1;32mc:\\Users\\NIPEAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\NIPEAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\NIPEAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x00007FF750ACFDA5+29557]\n\t(No symbol) [0x00007FF750A42240]\n\t(No symbol) [0x00007FF7508FB6EA]\n\t(No symbol) [0x00007FF7508CFCD5]\n\t(No symbol) [0x00007FF75097F167]\n\t(No symbol) [0x00007FF7509981A1]\n\t(No symbol) [0x00007FF7509772A3]\n\t(No symbol) [0x00007FF7509412DF]\n\t(No symbol) [0x00007FF750942451]\n\tGetHandleVerifier [0x00007FF750DFDCBD+3363469]\n\tGetHandleVerifier [0x00007FF750E49B47+3674391]\n\tGetHandleVerifier [0x00007FF750E3EAEB+3629243]\n\tGetHandleVerifier [0x00007FF750B8FC66+815670]\n\t(No symbol) [0x00007FF750A4D6EF]\n\t(No symbol) [0x00007FF750A492B4]\n\t(No symbol) [0x00007FF750A49450]\n\t(No symbol) [0x00007FF750A381FF]\n\tBaseThreadInitThunk [0x00007FFAC2DA7374+20]\n\tRtlUserThreadStart [0x00007FFAC315CC91+33]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Path to your downloaded ChromeDriver\n",
    "chrome_driver_path =\"C:/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "# Set up the WebDriver service\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the Indeed page with Selenium\n",
    "url = \"https://www.indeed.com/jobs?q=python&l=Remote&ts=1727179231724&from=searchOnHP&rq=1&rsIdx=0&fromage=last&vjk=52108bd7282a4c2d\"\n",
    "driver.get(url)\n",
    "\n",
    "# Allow time for the page to fully load\n",
    "time.sleep(5)\n",
    "\n",
    "# Find all job titles on the page\n",
    "job_titles = driver.find_elements(By.CLASS_NAME, 'jobTitle')\n",
    "\n",
    "# Extract and print job titles\n",
    "job_title_list = [title.text for title in job_titles]\n",
    "for job in job_title_list:\n",
    "    print(job)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Set the path to the ChromeDriver\n",
    "chrome_driver_path = \"C:/chromedriver-win64/chromedriver.exe\"\n",
    "# Set up the WebDriver service\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open a website (e.g., Indeed)\n",
    "url = \"https://www.indeed.com/jobs?q=python&l=Remote\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nile Bits, LLC -', 'Pyramids Pharmacy  -', 'coto -', 'Afaqy -', 'RDI -', 'RDI -', 'Bright Creations -', 'Al Soug-Cashi -', 'Storlytics  -', 'RDI -', 'CodeTreps -', 'CONSULTIX  -', 'willys kitchen-Egypt -', 'EpsilonAI -', 'Al Ateeq Software -']\n",
      "\n",
      "['Senior Python Developer', 'Full Stack Developer - Python', 'Python Artisan Developer', 'Python Developer', 'Full-Stack Python Team Leader (Django/React)', 'Senior Full-Stack Python (Django/React) in KSA', 'DAZU Pay - Python Developer (FinTech)', 'Junior data engineer', 'Senior Renewable Energy Engineer', 'Junior Python Developer', 'Instructor for Kids , ages 7-17 (with focus on schools)', 'Full Stack Software Engineer - Internet of Things (IoT)', 'Senior Odoo Developer', 'Data Science Instructor', 'Data Scientist']\n",
      "\n",
      "['Full TimeRemoteExperienced · 3 - 10 Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · AWS · APIs · RESTful · Python · Software Engineering', 'Full TimeHybridExperienced · 2 - 5 Yrs of Exp · IT/Software Development · Python · Django · React · AWS · Docker · Github', 'Full TimeRemoteExperienced · 3 - 6 Yrs of Exp · IT/Software Development · Django · API · AI · Data Processing · Computer Science · Docker · Engineering · Information Technology (IT) · MongoDB', 'Full TimeHybridExperienced · 1 - 3 Yrs of Exp · IT/Software Development · Computer Science · Software Development · Python', 'Full TimeOn-siteExperienced · 5 - 8 Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · React · Django · Full Stack Development · Information Technology (IT) · Computer Science · RESTful API · Microservice Architecture · Database Systems', 'Full TimeOn-siteExperienced · 5 - 8 Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · React · Django · Full Stack Development · Information Technology (IT) · Computer Science · RESTful API · Microservice Architecture · Database Systems', 'Full TimeFreelance / ProjectHybridEntry Level · 2+ Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · Python · Django · Flask · Python Scripting · Web Scraping · WWeb Scraping · CI · Computer Science', 'Full TimeOn-siteEntry Level · 1 - 3 Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · AWS · BI · Data · Engineering · Python · SQL · Database · Software Development', 'Full TimeRemoteExperienced · 3 - 10 Yrs of Exp · Engineering - Mechanical/Electrical · English · AutoCAD · Electrical Engineering · Renewable Energy · Power · PVsyst · Python', 'Full TimeHybridEntry Level · 1 - 3 Yrs of Exp · Business Development · IT/Software Development · Engineering - Telecom/Technology · Python · MySQL · JavaScript · React · Angular · RESTful · Django', 'Full TimePart TimeOn-siteEntry Level · 1 - 3 Yrs of Exp · IT/Software Development · Education/Teaching · Training/Instructor · Computer Science · Entrepreneurship · Education · Programming · Teaching · Training · Instructor', 'Full TimeOn-siteExperienced · 3 - 5 Yrs of Exp · IT/Software Development · Computer Science · Full Stack · Information Technology (IT) · JavaScript · Software · Software Development · SQL · Python · Development', 'Full TimeOn-siteExperienced · 5 - 7 Yrs of Exp · males_only · IT/Software Development · Engineering - Telecom/Technology · Information Technology (IT) · Software Development · Odoo · JavaScript · HTML · Python · XML · Software Engineering', 'Full TimePart TimeExperienced · 1+ Yrs of Exp · IT/Software Development · Engineering - Telecom/Technology · Training/Instructor · Data Science · Computer Science · Computer Engineering · Python · Machine Learning · Deep Learning · Artificial Intelligence (AI)', 'Full TimeOn-siteExperienced · 7 - 10 Yrs of Exp · IT/Software Development · Analyst/Research · Engineering - Telecom/Technology · Data Mining · Computer Science · Data Science · Information Technology (IT) · Python · SQL · Research']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "#fetch url\n",
    "result=requests.get(\"https://wuzzuf.net/search/jobs/?q=python&a=hpb\")\n",
    "\n",
    "#save page content/markup\n",
    "src= result.content\n",
    "\n",
    "#create soup object to parse content\n",
    "soup=BeautifulSoup(src,'lxml')\n",
    "\n",
    "\n",
    "job_title=[]\n",
    "company_name=[]\n",
    "location_name=[]\n",
    "skills=[]\n",
    "\n",
    "\n",
    "# job info\n",
    "job_titles=soup.find_all('h2',{'class':'css-m604qf'})\n",
    "\n",
    "company_names=soup.find_all('a',{'class':'css-17s97q8'})\n",
    "\n",
    "location_names=soup.find_all('span',{'class':'css-5wys0k'})\n",
    "\n",
    "job_skills=soup.find_all('div',{'class':'css-y4udm8'})\n",
    "\n",
    "\n",
    "# iterate over list to extract needed info into othter list\n",
    "for i in range(len(job_titles)):\n",
    "    job_title.append(job_titles[i].text)\n",
    "    company_name.append(company_names[i].text)\n",
    "    location_name.append(location_names[i].text)\n",
    "    skills.append(job_skills[i].text)\n",
    "    \n",
    "    \n",
    "print(company_name)\n",
    "print()\n",
    "print(job_title)\n",
    "print()\n",
    "print(skills)\n",
    "\n",
    "file_list=[job_title,company_name,location_name,skills]\n",
    "\n",
    "exported=zip_longest(*file_list)\n",
    "\n",
    "with open(f\"C:/Users/NIPEAL/OneDrive/Desktop/projectss/py_project_githubs/web_scraping/ws_jobs_web/jobstest.csv\" , 'w',encoding='utf-8') as f:\n",
    "    wr=csv.writer(f)\n",
    "    wr.writerow(['job title', 'company name', 'locaton','skills'])\n",
    "    wr.writerows(\n",
    "         exported\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "#fetch url\n",
    "result=requests.get(\"https://wuzzuf.net/search/jobs/?q=python&a=hpb\")\n",
    "\n",
    "#save page content/markup\n",
    "src= result.content\n",
    "\n",
    "#create soup object to parse content\n",
    "soup=BeautifulSoup(src,'lxml')\n",
    "\n",
    "\n",
    "job_title=[]\n",
    "company_name=[]\n",
    "location_name=[]\n",
    "skills=[]\n",
    "\n",
    "#links for every job to get deep description about it\n",
    "links=[]\n",
    "\n",
    "\n",
    "# job info\n",
    "job_titles=soup.find_all('h2',{'class':'css-m604qf'})\n",
    "\n",
    "company_names=soup.find_all('a',{'class':'css-17s97q8'})\n",
    "\n",
    "location_names=soup.find_all('span',{'class':'css-5wys0k'})\n",
    "\n",
    "job_skills=soup.find_all('div',{'class':'css-y4udm8'})\n",
    "\n",
    "\n",
    "# iterate over list to extract needed info into othter list\n",
    "for i in range(len(job_titles)):\n",
    "    job_title.append(job_titles[i].text)\n",
    "    links.append(job_titles[i].find('a').attrs['href'])\n",
    "    company_name.append(company_names[i].text)\n",
    "    location_name.append(location_names[i].text)\n",
    "    skills.append(job_skills[i].text)\n",
    "    \n",
    "    \n",
    "# iterate on every links to get more descriptions for each job\n",
    "for link in links:\n",
    "    result=requests.get(link)\n",
    "    src=result.content\n",
    "    soup=BeautifulSoup(src,'lxml')\n",
    "    salaries=soup.find_all('div',{'class':'css-rcl8e5'})\n",
    "    print(salaries)\n",
    "    \n",
    "\n",
    "\n",
    "# file_list=[job_title,company_name,location_name,skills,links]\n",
    "\n",
    "# exported=zip_longest(*file_list)\n",
    "\n",
    "# with open(f\"C:/Users/NIPEAL/OneDrive/Desktop/projectss/py_project_githubs/web_scraping/ws_jobs_web/jobstest.csv\" , 'w',newline='',encoding='utf-8') as f:\n",
    "#     wr=csv.writer(f)\n",
    "#     wr.writerow(['job title', 'company name', 'locaton','skills','links'])\n",
    "#     wr.writerows(\n",
    "#          exported\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://wuzzuf.net/jobs/p/gSlIpqMnrkqV-Senior-Python-Developer-Nile-Bits-LLC-Giza-Egypt'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3663947768.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    i=\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "i=\n",
    "# iterate on every links to get more descriptions for each job\n",
    "for link in links:\n",
    "    if i==0:\n",
    "        result=requests.get(r'https://wuzzuf.net/jobs/p/gSlIpqMnrkqV-Senior-Python-Developer-Nile-Bits-LLC-Giza-Egypt?o=1&l=sp&t=sj&a=python|search-v3|hpb')\n",
    "        src=result.content\n",
    "        soup=BeautifulSoup(src,'html.parser')\n",
    "        salaries = soup.prettify()\n",
    "        \n",
    "        print(salaries)\n",
    "        i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kunal Shah Founder : CRED, curious. CRED\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\"\"\"\n",
    "The By class in selenium.webdriver.common.by is used to locate elements within a web page. It provides a set of attributes that \n",
    "represent different strategies for locating elements. Here are some common attributes you can use with By:\n",
    "    By.ID: Locate an element by its ID attribute.\n",
    "    By.NAME: Locate an element by its name attribute.\n",
    "    By.XPATH: Locate an element using an XPath expression.\n",
    "    By.CSS_SELECTOR: Locate an element using a CSS selector.\n",
    "    By.CLASS_NAME: Locate an element by its class name.\n",
    "    By.TAG_NAME: Locate an element by its tag name.\n",
    "    By.LINK_TEXT: Locate a link element by its visible text.\n",
    "    By.PARTIAL_LINK_TEXT: Locate a link element by a partial match of its visible text.\n",
    "\"\"\"\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "#______________________________________________________________________________________________1_____________   \n",
    "# Creating a webdriver instance\n",
    "chrome_driver_path = \"C:/chromedriver-win64/chromedriver.exe\"\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# This instance will be used to log into LinkedIn\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Opening linkedIn's login page\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    " \n",
    "# waiting for the page to load\n",
    "time.sleep(5)\n",
    " \n",
    "# entering username \n",
    "username = driver.find_element(By.ID, \"username\")\n",
    " \n",
    "# In case of an error, try changing the element\n",
    "# tag used here.\n",
    " \n",
    "# Enter Your Email Address\n",
    "username.send_keys(\"\")  \n",
    " \n",
    "# entering password\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "# In case of an error, try changing the element \n",
    "# tag used here.\n",
    " \n",
    "# Enter Your Password\n",
    "pword.send_keys(\"\")        \n",
    " \n",
    "# Clicking on the log in button\n",
    "# Format (syntax) of writing XPath --> \n",
    "# //tagname[@attribute='value']\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "# In case of an error, try changing the\n",
    "# XPath used here.\n",
    "\n",
    "\n",
    "#______________________________________________________________________________________________2_____________   \n",
    "# Opening Kunal's Profile\n",
    "# paste the URL of Kunal's profile here\n",
    "profile_url = \"https://www.linkedin.com/in/kunalshah1/\"\n",
    " \n",
    "driver.get(profile_url)  \n",
    "driver\n",
    "\n",
    "\n",
    "\n",
    "#______________________________________________________________________________________________3_____________   \n",
    "#  Now, we need to scroll to the bottom. Here is the code to do that:\n",
    "start = time.time()\n",
    " \n",
    "# will be used in the while loop\n",
    "initialScroll = 0\n",
    "finalScroll = 1000\n",
    " \n",
    "while True:\n",
    "    driver.execute_script(f\"window.scrollTo({initialScroll},{finalScroll})\")\n",
    "    # this command scrolls the window starting from\n",
    "    # the pixel value stored in the initialScroll \n",
    "    # variable to the pixel value stored at the\n",
    "    # finalScroll variable\n",
    "    initialScroll = finalScroll\n",
    "    finalScroll += 1000\n",
    " \n",
    "    # we will stop the script for 3 seconds so that \n",
    "    # the data can load\n",
    "    time.sleep(3)\n",
    "    # You can change it as per your needs and internet speed\n",
    " \n",
    "    end = time.time()\n",
    " \n",
    "    # We will scroll for 20 seconds.\n",
    "    # You can change it as per your needs and internet speed\n",
    "    if round(end - start) > 20:\n",
    "        break\n",
    "    \n",
    "    \n",
    "#______________________________________________________________________________________________4_____________   \n",
    "# Extracting Data from the Profile\n",
    "# To extract data, firstly, store the source code of the web page in a variable. Then, use this source \n",
    "# code to create a Beautiful Soup object.\n",
    "\n",
    "\n",
    "src = driver.page_source\n",
    " \n",
    "# Now using beautiful soup\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "\n",
    "# Extracting the HTML of the complete introduction box\n",
    "# that contains the name, company name, and the location\n",
    "intro = soup.find('div', {'class': 'mt2 relative'})\n",
    "\n",
    "name=intro.find('h1').get_text().strip()\n",
    "works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'}).text.strip()\n",
    "location_loc = intro.find_all(\"span\", {'class': 'text-body-small inline t-black--light break-words'})[0].get_text().strip()\n",
    "print(\"Name -->\", name,\n",
    "      \"\\nWorks At -->\", works_at_loc,\n",
    "      \"\\nLocation -->\", location_loc)\n",
    "\n",
    "\n",
    "# Getting the HTML of the Experience section in the profile\n",
    "experience = soup.find_all(\"section\", {\"class\": \"artdeco-card pv-profile-card break-words mt2\"})[1].find('ul')\n",
    "\n",
    "\n",
    "# exp=soup.find(\"section\", {\"class\": \"artdeco-card pv-profile-card break-words mt2\"}).find_all('ul')\n",
    "# print(experience==exp[0])  #True\n",
    "\n",
    "#first exp find to simplify\n",
    "print(experience.find('div').find_all('span')[0].text.strip())\n",
    "print(experience.find('div').find_all('span')[3].text.strip())\n",
    "print(experience.find('div').find_all('span')[6].text.strip())\n",
    "\n",
    "\n",
    "for ex in experience.find_all('div',{'class':\"OdisdwCWMtMIJFjwkkjUxJQnurvqaXgQaOX vsAATKtKqqHJBCXrRWgKkmUUPJwuUuo NwliYhHDezFmMFZUqschmUCeDCUsgXzjQoo\"}):\n",
    "    print(ex.find_all('span')[0].text.strip())\n",
    "    print(ex.find_all('span')[3].text.strip())\n",
    "    print(ex.find_all('span')[5].text.strip())\n",
    "    print(\"__________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent Director Syrma SGS Technology LimitedSyrma SGS Technology Limited Nov 2021 - Present · 2 yrs 11 mosNov 2021 to Present · 2 yrs 11 mos\n",
      "Independent DirectorIndependent Director\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Syrma SGS Technology LimitedSyrma SGS Technology Limited\n",
      "\n",
      "\n",
      "Nov 2021 - Present · 2 yrs 11 mosNov 2021 to Present · 2 yrs 11 mos ______________\n",
      "\n",
      "<div class=\"ivm-view-attr__img-wrapper\">\n",
      "<!-- -->\n",
      "<!-- --> <img alt=\"Syrma SGS Technology Limited logo\" class=\"ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view\" height=\"48\" id=\"ember43\" loading=\"lazy\" src=\"https://media.licdn.com/dms/image/v2/C560BAQFz26EEjF1a-Q/company-logo_100_100/company-logo_100_100/0/1638322241902/syrma_technology_logo?e=1735171200&amp;v=beta&amp;t=nxqrRrLvOktYge3-ibdltVI_GjkyvvmlcnPmFC5Q6Qg\" width=\"48\"/>\n",
      "</div> ++++++++++\n",
      "Independent DirectorIndependent Director\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Pine LabsPine Labs\n",
      "\n",
      "\n",
      "Aug 2021 - Present · 3 yrs 2 mosAug 2021 to Present · 3 yrs 2 mos ______________\n",
      "\n",
      "<div class=\"ivm-view-attr__img-wrapper\">\n",
      "<!-- -->\n",
      "<!-- --> <img alt=\"Pine Labs logo\" class=\"ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view\" height=\"48\" id=\"ember44\" loading=\"lazy\" src=\"https://media.licdn.com/dms/image/v2/C4D0BAQEA5BfWRWkiog/company-logo_100_100/company-logo_100_100/0/1668430958333/pinelabs_logo?e=1735171200&amp;v=beta&amp;t=MqLLTLIPNqyn5P57ugzr7mhxSmlIXmlF1_pb2pu1-hc\" width=\"48\"/>\n",
      "</div> ++++++++++\n",
      "FounderFounder\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "CREDCRED\n",
      "\n",
      "\n",
      "Apr 2018 - Present · 6 yrs 6 mosApr 2018 to Present · 6 yrs 6 mos\n",
      "\n",
      "\n",
      "Bengaluru Area, IndiaBengaluru Area, India ______________\n",
      "\n",
      "<div class=\"ivm-view-attr__img-wrapper\">\n",
      "<!-- -->\n",
      "<!-- --> <img alt=\"CRED logo\" class=\"ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view\" height=\"48\" id=\"ember45\" loading=\"lazy\" src=\"https://media.licdn.com/dms/image/v2/C560BAQFTPHiIlzOnYw/company-logo_100_100/company-logo_100_100/0/1660666593542/credapp_logo?e=1735171200&amp;v=beta&amp;t=E-zXptji43TX4O02KjIJfjLnv1pskIIeVA-NXx80uHY\" width=\"48\"/>\n",
      "</div> ++++++++++\n",
      "Advisor To The BoardAdvisor To The Board\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Bennett Coleman and Co. Ltd. (Times Group)Bennett Coleman and Co. Ltd. (Times Group)\n",
      "\n",
      "\n",
      "Oct 2017 - Present · 7 yrsOct 2017 to Present · 7 yrs\n",
      "\n",
      "\n",
      "IndiaIndia ______________\n",
      "\n",
      "<div class=\"ivm-view-attr__img-wrapper\">\n",
      "<!-- -->\n",
      "<!-- --> <img alt=\"Bennett Coleman &amp; Co. Ltd. (The Times of India) logo\" class=\"ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view\" height=\"48\" id=\"ember46\" loading=\"lazy\" src=\"https://media.licdn.com/dms/image/v2/D4D0BAQGuuowCE8n9GQ/company-logo_100_100/company-logo_100_100/0/1689834630986/bennett_coleman_and_co_ltd_times_group__logo?e=1735171200&amp;v=beta&amp;t=TVsHu5NUjTvPQtL3qQyum8kkvRYgcWyf3wMa-sJk1ro\" width=\"48\"/>\n",
      "</div> ++++++++++\n",
      "AdvisorAdvisor\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "AngelListAngelList\n",
      "\n",
      "\n",
      "Jan 2018 - Present · 6 yrs 9 mosJan 2018 to Present · 6 yrs 9 mos\n",
      "\n",
      "\n",
      "GlobalGlobal ______________\n",
      "\n",
      "<div class=\"ivm-view-attr__img-wrapper\">\n",
      "<!-- -->\n",
      "<!-- --> <img alt=\"AngelList logo\" class=\"ivm-view-attr__img--centered EntityPhoto-square-3 evi-image lazy-image ember-view\" height=\"48\" id=\"ember47\" loading=\"lazy\" src=\"https://media.licdn.com/dms/image/v2/C4E0BAQHK8OClv0DLwA/company-logo_100_100/company-logo_100_100/0/1668476594231/angellist_logo?e=1735171200&amp;v=beta&amp;t=wsO_wP2ByojvtRDoLciy3qX5NTaWhkFBqPMH6Y72KSA\" width=\"48\"/>\n",
      "</div> ++++++++++\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it look like every search text it split and %20 contcate between \n",
    "# https://www.linkedin.com/search/results/all/?keywords=python%20develope&origin=AUTO_COMPLETE&searchId=d3032ec4-e4b9-4eca-a57a-ff5119b7cd97&sid=RCj&spellCorrectionEnabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent Director\n",
      "Syrma SGS Technology Limited\n",
      "Nov 2021 - Present · 2 yrs 11 mos\n",
      "Independent Director\n",
      "Syrma SGS Technology Limited\n",
      "Nov 2021 - Present · 2 yrs 11 mosNov 2021 to Present · 2 yrs 11 mos\n",
      "__________________\n",
      "Independent Director\n",
      "Pine Labs\n",
      "Aug 2021 - Present · 3 yrs 2 mosAug 2021 to Present · 3 yrs 2 mos\n",
      "__________________\n",
      "Founder\n",
      "CRED\n",
      "Apr 2018 - Present · 6 yrs 6 mosApr 2018 to Present · 6 yrs 6 mos\n",
      "__________________\n",
      "Advisor To The Board\n",
      "Bennett Coleman and Co. Ltd. (Times Group)\n",
      "Oct 2017 - Present · 7 yrsOct 2017 to Present · 7 yrs\n",
      "__________________\n",
      "Advisor\n",
      "AngelList\n",
      "Jan 2018 - Present · 6 yrs 9 mosJan 2018 to Present · 6 yrs 9 mos\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "# Getting the HTML of the Experience section in the profile\n",
    "experience = soup.find_all(\"section\", {\"class\": \"artdeco-card pv-profile-card break-words mt2\"})[1].find('ul')\n",
    "\n",
    "\n",
    "# exp=soup.find(\"section\", {\"class\": \"artdeco-card pv-profile-card break-words mt2\"}).find_all('ul')\n",
    "# print(experience==exp[0])  #True\n",
    "\n",
    "#first exp find to simplify\n",
    "print(experience.find('div').find_all('span')[0].text.strip())\n",
    "print(experience.find('div').find_all('span')[3].text.strip())\n",
    "print(experience.find('div').find_all('span')[6].text.strip())\n",
    "\n",
    "\n",
    "for ex in experience.find_all('div',{'class':\"OdisdwCWMtMIJFjwkkjUxJQnurvqaXgQaOX vsAATKtKqqHJBCXrRWgKkmUUPJwuUuo NwliYhHDezFmMFZUqschmUCeDCUsgXzjQoo\"}):\n",
    "    print(ex.find_all('span')[0].text.strip())\n",
    "    print(ex.find_all('span')[3].text.strip())\n",
    "    print(ex.find_all('span')[5].text.strip())\n",
    "    print(\"__________________\")\n",
    "                      \n",
    "                      \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
